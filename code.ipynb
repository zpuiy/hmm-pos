{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhXw1yia4i9Z"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from os import unlink\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9zvSG1a4qlJ",
        "outputId": "c21eb229-1fb7-4972-af36-2372979810c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFT0bdiP4tMZ"
      },
      "outputs": [],
      "source": [
        "# load all data\n",
        "with open('/content/drive/MyDrive/CSCI544/CSCI544_HW2/data/train.json') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/CSCI544/CSCI544_HW2/data/dev.json') as f:\n",
        "    dev_data = json.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/CSCI544/CSCI544_HW2/data/test.json') as f:\n",
        "    test_data = json.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LA6JBDB8IY5"
      },
      "source": [
        "task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh15uNuW-cC4",
        "outputId": "0b0bba84-7912-4d57-c36f-fc59ff51c913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold: 2\n",
            "vocab size: 23183\n",
            "unknonwn frequency: 20011\n"
          ]
        }
      ],
      "source": [
        "vocab_dict = {}\n",
        "\n",
        "labels = set()\n",
        "for item in train_data:\n",
        "    sentence = item['sentence']\n",
        "    labels.update(item['labels'])\n",
        "    for word in sentence:\n",
        "        vocab_dict[word] = vocab_dict.get(word, 0) + 1\n",
        "\n",
        "new_dict = {}\n",
        "thres = 2\n",
        "print(f'Threshold: {thres}')\n",
        "unk_freq = 0\n",
        "unk_words = []\n",
        "for word, freq in vocab_dict.items():\n",
        "    if freq < thres:\n",
        "        unk_freq = unk_freq + 1\n",
        "        unk_words.append(word)\n",
        "    else:\n",
        "        new_dict[word] = freq\n",
        "\n",
        "print(f'vocab size: {len(new_dict) + 1}')\n",
        "print(f'unknonwn frequency: {unk_freq}')\n",
        "\n",
        "vocab = sorted(new_dict.items(), key=lambda x: x[1], reverse=False)\n",
        "# array for all words in vocab\n",
        "unique_words = np.array(list(new_dict.keys()))\n",
        "unique_words = np.insert(unique_words, 0, '<unk>')\n",
        "\n",
        "# create a dictionary to store word indices\n",
        "word_index = {}\n",
        "for i in range(len(unique_words)):\n",
        "    word_index[unique_words[i]] = i\n",
        "\n",
        "# array for all tags\n",
        "tags = np.array(list(labels))\n",
        "\n",
        "# create a dictionary to store word indices\n",
        "tag_index = {}\n",
        "for i in range(len(tags)):\n",
        "    tag_index[tags[i]] = i\n",
        "\n",
        "vocab_txt = []\n",
        "for i in range(len(vocab)):\n",
        "    vocab_txt.append(f'{vocab[i][0]}\\t{i+1}\\t{vocab[i][1]}')\n",
        "\n",
        "with open('/content/drive/MyDrive/CSCI544/CSCI544_HW2/vocab.txt', 'w') as f:\n",
        "    f.write(f'<unk>\\t0\\t{unk_freq}\\n')\n",
        "    f.writelines('\\n'.join(vocab_txt))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ori_test = copy.deepcopy(test_data)\n",
        "# replace unknown words\n",
        "def replace_unknown(data):\n",
        "    new = data.copy()\n",
        "    for item in new:\n",
        "        item['sentence'] = ['<unk>' if word not in word_index else word for word in item['sentence']]\n",
        "    return new\n",
        "\n",
        "\n",
        "new_train = replace_unknown(train_data)\n",
        "new_test = replace_unknown(test_data)\n",
        "new_dev = replace_unknown(dev_data)\n"
      ],
      "metadata": {
        "id": "oJzQA3z033GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pire1fN3JPle"
      },
      "source": [
        "task 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YklTZD2F8HpI"
      },
      "outputs": [],
      "source": [
        "# initialize transition parameter matrix\n",
        "transition = np.zeros((len(labels), len(labels)))\n",
        "\n",
        "count_null_s = np.zeros(len(labels))\n",
        "count_s_s = np.zeros((len(labels), len(labels)))\n",
        "count_s = np.zeros(len(labels))\n",
        "\n",
        "# count_x_s = np.zeros()\n",
        "\n",
        "for item in new_train:\n",
        "    curr_labels = item['labels']\n",
        "    count_null_s[tag_index.get(curr_labels[0])] += 1\n",
        "    count_s[tag_index.get(curr_labels[0])] += 1\n",
        "    for i in range(0, len(curr_labels) - 1):\n",
        "        index_s = tag_index.get(curr_labels[i])\n",
        "        index_s_to = tag_index.get(curr_labels[i+1])\n",
        "        count_s_s[index_s][index_s_to] += 1\n",
        "        count_s[index_s_to] += 1\n",
        "\n",
        "# count_s = np.sum(count_s_s, axis=1)\n",
        "transition = count_s_s / count_s[:, np.newaxis]\n",
        "prior = count_null_s / len(new_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvBvwkUhcYvL"
      },
      "outputs": [],
      "source": [
        "# create emission matrix\n",
        "emission = np.zeros((len(labels), len(unique_words)))\n",
        "\n",
        "count_x_s = np.zeros((len(labels), len(unique_words)))\n",
        "count_s = np.zeros(len(labels))\n",
        "\n",
        "for item in new_train:\n",
        "    curr_labels = item['labels']\n",
        "    curr_sen = item['sentence']\n",
        "    for i in range(len(curr_labels)):\n",
        "        index_s = tag_index.get(curr_labels[i])\n",
        "        count_s[index_s] += 1\n",
        "        index_x = word_index.get(curr_sen[i])\n",
        "        count_x_s[index_s][index_x] = count_x_s[index_s][index_x] + 1\n",
        "\n",
        "emission = count_x_s / count_s[:, np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create output file\n",
        "initial_dict = {}\n",
        "for i in range(len(tags)):\n",
        "    initial_dict[tags[i]] = prior[i]\n",
        "\n",
        "transition_dict = {}\n",
        "for i in range(len(transition)):\n",
        "    curr = transition[i]\n",
        "    for j in range(len(curr)):\n",
        "        transition_dict[f'({tags[i]}, {tags[j]})'] = transition[i][j]\n",
        "\n",
        "emission_dict = {}\n",
        "for i in range(len(emission)):\n",
        "    curr = emission[i]\n",
        "    for j in range(len(curr)):\n",
        "        emission_dict[f'({tags[i]}, {unique_words[j]})'] = emission[i][j]\n",
        "\n",
        "hmm = {\"initial\": initial_dict, \"transition\": transition_dict, \"emission\": emission_dict}\n",
        "\n",
        "# write learned model into a model file in json format, named hmm.json.\n",
        "with open('./hmm.json', 'w') as f:\n",
        "    json.dump(hmm, f)"
      ],
      "metadata": {
        "id": "vKPQ6K34b9Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "task 3\n"
      ],
      "metadata": {
        "id": "AfKKTmuOLEZY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T0EFmZ8Udao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b9172e-108c-401c-8364-a2afc0fb2b79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for greedy decoding: 0.9350297492562686\n"
          ]
        }
      ],
      "source": [
        "# greedy decoding\n",
        "def greedy(sentence):\n",
        "    result = []\n",
        "    w_index = word_index.get(sentence[0])\n",
        "    prev_y = np.argmax(prior * emission[:, w_index])\n",
        "    result.append(tags[prev_y])\n",
        "    for i in range(1, len(sentence)) :\n",
        "        w_index = word_index.get(sentence[i])\n",
        "        curr_y = np.argmax(transition[prev_y] * emission[:,w_index])\n",
        "        prev_y = curr_y\n",
        "        result.append(tags[curr_y])\n",
        "    return result\n",
        "\n",
        "# create a function to calculate accuracy\n",
        "def evaluate(model):\n",
        "    n_true = 0\n",
        "    n_total = 0\n",
        "    for item in new_dev:\n",
        "        curr_sen = item['sentence']\n",
        "        curr_pred = model(curr_sen)\n",
        "        true_pred = item['labels']\n",
        "        n_total += len(curr_sen)\n",
        "        for i in range(len(curr_sen)):\n",
        "            if true_pred[i] == curr_pred[i]:\n",
        "                n_true += 1\n",
        "\n",
        "    return n_true / n_total\n",
        "\n",
        "# evaluate it on the development data\n",
        "greedy_accuracy = evaluate(greedy)\n",
        "print(f'Accuracy for greedy decoding: {greedy_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction on test data\n",
        "def predict(model):\n",
        "    result = copy.deepcopy(ori_test)\n",
        "    for i in range(len(new_test)):\n",
        "        curr_sen = new_test[i]['sentence']\n",
        "        ori_test[i]['labels'] = greedy(curr_sen)\n",
        "    return result\n",
        "\n",
        "with open('./greedy.json', 'w') as f:\n",
        "        json.dump(predict(greedy), f)"
      ],
      "metadata": {
        "id": "aMs6DuLfZBCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "task 4"
      ],
      "metadata": {
        "id": "1L9Au2XVwPNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi(sentence):\n",
        "    T1 = np.zeros((len(tags), len(sentence)), dtype=float)\n",
        "    T2 = np.zeros((len(tags), len(sentence)), dtype=int)\n",
        "    for s in range(len(tags)):\n",
        "        index_x = word_index.get(sentence[0])\n",
        "        T1[s][0] = prior[s] * emission[s][index_x]\n",
        "\n",
        "    for o in range(1, len(sentence)):\n",
        "        for s in range(len(tags)):\n",
        "            index_x = word_index.get(sentence[o])\n",
        "            k = np.argmax(T1[:,o-1]*transition[:,s]*emission[s,index_x])\n",
        "            T1[s,o] = T1[k][o-1]*transition[k,s]*emission[s,index_x]\n",
        "            T2[s,o] = k\n",
        "\n",
        "    best_path = []\n",
        "    k = np.argmax(T1[:,len(sentence) - 1])\n",
        "    for o in range(len(sentence) -1, -1, -1):\n",
        "        best_path.insert(0,tags[k])\n",
        "        k = T2[k,o]\n",
        "    return best_path\n",
        "\n",
        "\n",
        "# evaluate it on the development data\n",
        "viterbi_accuracy = evaluate(viterbi)\n",
        "print(f'Accuracy for viterbi decoding: {viterbi_accuracy}')"
      ],
      "metadata": {
        "id": "3e-dCKbmwP9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07c4592-42d1-42c4-9c72-06464a52d7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for greedy decoding: 0.9476883613623945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make prediction on test data\n",
        "with open('./greedy.json', 'w') as f:\n",
        "        json.dump(predict(viterbi), f)"
      ],
      "metadata": {
        "id": "t4aHFM5tWmXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}